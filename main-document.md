# NIH’s NCATS challenge: create a solution that detects bias in AI/ML models used in clinical decisions. 

## Some ideas to consider:

1.  How do you identify ***predictive*** and ***social*** bias?
2.  How do you account for “latent” bias where social or statistical biases happen over time due to the complexities of healthcare processes?
3.  Where does bias occur and how do we provide a path forward for follow-up investigations?
4.  How do you account for consistent evaluation and assessments of the algorithm over time and for all patient populations?

## Bias Types (defined by NCATS challenge)

1. **Predictive Bias**: Algorithmic inaccuracies in producing estimates that significantly differ from the underlying truth.
2. **Social Bias**: Systemic inequities in care delivery leading to suboptimal health outcomes for certain populations.

## Reason for inaccurate or unreliable AI/ML model/algo over time due to various factors; 

1. changes in data distribution
2. subtle shifts in the data
3. real world interactions
4. user behavior
5. shifts in data capture and management practices
